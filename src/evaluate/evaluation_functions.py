#!/usr/bin/env python3
"""
Project title:       CollembolAI
Authors:             Stephan Weißbach, Stanislav Sys, Clément Schneider
Original repository: https://github.com/stasys-hub/Collembola_AI.git
Module title:        match_groundtruth.py
Purpose:             match a given result json file with the groundtruth (test.json)
Dependencies:        See ReadMe
Last Update:         18.02.2022
"""

import pandas as pd
from itertools import product
import json
import numpy as np
import os

from cocoutils import *
from nms import *
from output_inference_images import *
from third_party_utils import plot_confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt


def match_true_n_pred_box(df_ttruth, df_pred, IoU_threshold=0.4):
    """Match the ground truth annotations with the predicted annotations based on IoU, then merge ground truth
    and prediction dataframe on shared annotation, and output the merged dataframe"""
    matched = pd.DataFrame()
    df_pred["id_pred"] = df_pred["id"]
    df_pred["pred_box"] = df_pred["box"]
    df_ttruth["id_true"] = df_ttruth["id"]
    df_ttruth["true_box"] = df_ttruth["box"]
    df_ttruth["true_area"] = df_ttruth["area"]

    for image_id in df_pred.file_name.unique():
        # subset dataframe to only have predictions of one image
        sdf_pred = df_pred[df_pred["file_name"] == image_id]
        sdf_ttruth = df_ttruth[df_ttruth["file_name"] == image_id]
        sdf_ttruth = df_ttruth[df_ttruth["file_name"] == image_id]
        # create one df with all possible combinations of predicted and groundtruth boxes
        df = pd.DataFrame(
            product(sdf_ttruth["id"], sdf_pred["id"]), columns=["id_true", "id_pred"]
        )
        # add information from original dataframes
        df = df.merge(
            sdf_ttruth[["id_true", "true_box", "true_area"]], how="left", on="id_true"
        ).merge(sdf_pred[["id_pred", "pred_box", "score"]], how="left", on="id_pred")
        # compute intersection, union and IoU
        df["intersection"] = df[["true_box", "pred_box"]].apply(
            lambda x: x[0].intersection(x[1]).area, axis=1
        )
        df["union"] = df[["true_box", "pred_box"]].apply(
            lambda x: x[0].union(x[1]).area, axis=1
        )

        df["IoU"] = df["intersection"] / df["union"]
        # filter for boxes that are below IoU threshold
        df = df[df["IoU"] > IoU_threshold]
        # concat
        matched = pd.concat([matched, df], axis=0)
    # keep only best (by confidence score) predictions for each bbox
    df2 = matched.sort_values(by="score", ascending=False)
    df2 = df2.drop_duplicates(subset=["id_pred"], keep="first")
    df2 = df2.drop_duplicates(subset=["id_true"], keep="first")
    # add information about correctness of prediction
    pairs = (
        df_ttruth[["id_true", "name"]]
        .merge(df2[["id_true", "id_pred"]], how="left", on="id_true")
        .merge(df2[["id_pred", "score"]], how="outer", on="id_pred")
        .rename(columns={"name": "name_true"})
    )
    pairs = pairs.merge(df_pred[["id_pred", "name", "score"]], how="outer", on="id_pred").rename(
        columns={"name": "name_pred"}
    )
    pairs['score'] = pairs['score_x'].where(pairs['score_x'].notnull(), pairs['score_y'])
    pairs["is_correct"] = pairs["name_true"] == pairs["name_pred"]
    pairs["is_correct_class"] = (pairs["name_true"] == pairs["name_pred"]).where(
        pairs.id_pred.notnull(), np.nan
    )
    return pairs

def IoU(row):
    '''Compute the IoU of truth/pred boxes in our matching dataframe'''
    return row[0].intersection(row[1]).area / row[0].union(row[1]).area

def precision(tp: int, fp: int) -> float:
    """
    tp: number of True Positives
    fp: number of False Positives
    """
    # handle potental error
    if (tp + fp) == 0:
        return 0
    else:
        return tp/(tp + fp)

def recall(true_positive: int, total_possible_positives: int) -> float:
    """
    true_positive: number of True Positives
    total_possible_positives: TP + FN
    """
    # avoid division by 0
    if total_possible_positives == 0:
        return 0
    else:
        return true_positive/total_possible_positives

def get_status_count(df_pairs):
    """
    Count number of TP, FP and FN from the true/pred dataframe generated by function 'process_results' and return as dataframe of outcomes per class
    """
    cls_value_counts = pd.concat([
        df_pairs.groupby('name_true')['id_true'].count().rename('Ground truth total'),
        df_pairs[df_pairs['status'] == 'True Positive'].groupby('name_true')['id_true'].count().rename('True positive'),
        df_pairs[df_pairs['status'] == 'False positive (background)'].groupby('name_pred')['id_pred'].count().rename('False positive background'),
        df_pairs[df_pairs['status'] == 'False negative (not detected)'].groupby('name_true')['id_true'].count().rename('False negative (not detected)'),
        df_pairs[df_pairs['status'] == 'Missclassified'].groupby('name_true')['id_true'].count().rename('False negative (from missclassification)'),
        df_pairs[df_pairs['status'] == 'Missclassified'].groupby('name_pred')['id_pred'].count().rename('False positive (from missclassification)')], axis=1).fillna(0)
    return cls_value_counts

def get_average_precision_recall_from_cls_vc(cls_value_counts):
    """ Return the macro average and the micro average recall and precision from the outcome dataframe (this is at a given IoU and
    and score threshold).
        Macro-average computes the metric independently for each class and then take the average.
        Micro-average aggregates the contributions of all classes to compute the average metric.
    """

    # Micro metrics
    sum_value_counts = cls_value_counts.sum(axis=0)
    up = precision(
            sum_value_counts['True positive'],
            (sum_value_counts['False positive background'] +
            sum_value_counts['False positive (from missclassification)'])
            )
    ur = recall(sum_value_counts['True positive'], sum_value_counts['Ground truth total'])

    # Macro metrics
    mp = (cls_value_counts['True positive'] /
         (cls_value_counts['True positive'] +
         cls_value_counts['False positive background'] +
         cls_value_counts['False positive (from missclassification)'])).sum() / cls_value_counts.shape[0]
    mr = (cls_value_counts['True positive'] / cls_value_counts['Ground truth total']).sum() / cls_value_counts.shape[0]
    return up, ur, mp, mr

def get_mAP_from_TruthPred_df(pairs, verbose=True):
    """
    Compute the mAP, as per the Pascal VOC competition standard, over all values of Recall.
    'pairs' is the truth/pred DF outputed by function 'process_results'. Note that the mAP IoU threshold is equal to the parameter 'match_thresh'
    passed to 'process_results'
    """

    df_new = pairs.sort_values("score", ascending=False)
    # Remap the long FP,TP etc Definitions ->
    remap_values_dict = {'True Positive': 'TP',
                         'False negative (not detected)': 'FN',
                         'Missclassified': 'MIS',
                         'False positive (background)':'FP' }
    df_new = df_new.replace({"status": remap_values_dict})

    # calculate metrics per class
    recall_per_class = []
    precision_per_class = []
    classs_list = []

    # get class names (including nan)
    for i in df_new["name_true"].unique():
        # subset by class

        TOTAL_GROUND_TRUTH = df_new[df_new["name_true"] == i].shape[0]

        tmp_df = df_new[(df_new["name_true"] == i) | (df_new["name_pred"] == i)] # selecting all rows where class i is mentionned
        tmp_df = tmp_df.sort_values("score", ascending=True)

        # get the amount of FN that never got detected (score = NaN, status = FN). This number is never going to change
        # Then drop those rows.
        always_false_negative = tmp_df[tmp_df['status'] == 'FN'].shape[0]
        tmp_df = tmp_df[tmp_df['status'] != 'FN']
        total_predictions = tmp_df.shape[0]

        # Create objects to collect metrics per class
        recall_list = []
        precision_list = []

        # Adding predictions one by one, from highest score to lowest
        for idx in range(total_predictions-1, 0-1, -1):
            subtmp_df = tmp_df.iloc[idx:total_predictions]
            true_positive = subtmp_df[subtmp_df['status'] == 'TP'].shape[0]
            false_positive = subtmp_df[subtmp_df['name_true'] != i].shape[0] # FP when the true name is not from class i (i.e, pred name is incorrectly from class i)
            false_negative = subtmp_df[subtmp_df['name_pred'] != i].shape[0] # FN when the predicted name is not from class i
            false_negative = false_negative + always_false_negative # FN are not useful for calculation, but still could be printed.

            # calculate metrics, store in lists
            recall_list.append(recall(true_positive, TOTAL_GROUND_TRUTH ))
            precision_list.append(precision(true_positive, false_positive))

        # append data for every class
        recall_per_class.append(recall_list)
        precision_per_class.append(precision_list)
        classs_list.append(i)

        #print("Species: ", i)
        #print("Total Detections (TP+FP): ", true_positive+false_positive)
        #print("TP: ", true_positive)
        #print("FP: ", false_positive)
        #print("FN: ", false_negative)

    # accumulated AP for all classes
    AP_acc = 0
    #AP_per_class = [] # CLEM: This variable is never used ???

    # perform AP calulation and plotting for every class
    for idx, species in enumerate(classs_list):
        # check if precision and recall have the same dims
        if len(recall_per_class[idx]) != len(precision_per_class[idx]):
            print("Dimension of recall and precision table do not fit!!")
        else:
            # transform to np arrays and prepend 0 and 1 repectively for easy recursive calculation
            recalls = np.append(0,np.array(recall_per_class[idx]))
            precisions = np.append(1,np.array(precision_per_class[idx]))

            for i in range(0,len(precisions)):
                precisions[i] = max(precisions[i:])

            # Dropping duplicated recall values if any
            tdf=pd.DataFrame(data={'recalls':recalls,'precisions':precisions}).drop_duplicates(subset='recalls', keep='first')
            recalls = tdf['recalls'].values
            precisions = tdf['precisions'].values

            # calc AP per class (PASCAL VOC)
            AP = np.sum((recalls[1:] - recalls[:-1]) * precisions[:-1])
            if verbose:
                print(f"{species} AP: {AP}")
            AP_acc += AP

    mAP = AP_acc/(len(classs_list)-1)
    return mAP

def process_results(test_directory, output_directory, train_directory, nms_IoU=0.15,
                    match_thresh=0.4, write_outputs=True, verbose=True, draw_n_plot=False, score_thresh=0.7):
    """
    This block of code originally came from the CollembolAI.py script, within the "start_evaluation_on_test" function.
    It parses the results from detectron2, apply a few obvious cleaning on them (dropping insanly large bounding box
    and remove duplicated boxes).
    Then it compares the predicted annotations with the true annotations in order to estimate accuracy.
    Predicted labels and True labels are matched based on a IoU criterion.
    Its output is necessary for evaluation.
    """

    with open(os.path.join(output_directory, "result.json"), 'r') as j:
        tpred = json.load(j)

    print('\n\nLoading predictions and conducting non max supression')

    df_pred = coco2df(tpred)
    # Dropping predictions with score below the score_threshold.
    df_pred = df_pred[df_pred['score'] >= score_thresh]

    if nms_IoU > 0:
        df_pred = non_max_supression(df_pred, IoU_threshold=nms_IoU)

    # Loading ground ttruth labels (test set)
    with open(os.path.join(test_directory, "test.json"), 'r') as j:
            ttruth =  json.load(j)
            df_ttruth = coco2df(ttruth)
            df_ttruth['id_true'] = df_ttruth['id']

    # Loading train set labels
    with open(os.path.join(train_directory, "train.json"), 'r') as j:
            train =  json.load(j)
            df_train = coco2df(train)
            df_train['id_train'] = df_train['id']

    # Reporting the abundance of each labels in the train set, and in the test pictures (true and predicted.)
    tt_abundances = df_train.name.value_counts().to_frame().join(df_ttruth.name.value_counts(), lsuffix='_train', rsuffix='_test')
    tt_abundances.columns = ['Train', 'Test']


    if verbose:
        print('\n\nAbundance and area of each species in the train and test pictures (true and predicted)\n')

    tt_abundances = tt_abundances.join(df_pred.name.value_counts())\
                            .join(df_ttruth.groupby('name').sum()['area'])\
                            .join(df_pred.groupby('name').sum()['area'], rsuffix="pred")
    tt_abundances.columns = ['Train', 'Test True', 'Test Pred', 'Test True Area', 'Test Pred Area']
    tt_abundances['Perc Pred True'] = tt_abundances['Test Pred Area'] / tt_abundances['Test True Area'] * 100
    tt_abundances['Test True Contribution To Total Area'] =  tt_abundances['Test True Area'] / tt_abundances['Test True Area'].sum() * 100
    tt_abundances['Test Pred Contribution To Total Area'] =  tt_abundances['Test Pred Area'] / tt_abundances['Test Pred Area'].sum() * 100
    if verbose:
        print(tt_abundances.to_markdown())
    if write_outputs:
        tmpoutdir = os.path.join(output_directory, "species_abundance_n_area.tsv")
        tt_abundances.to_csv(tmpoutdir, sep='\t')
        if verbose:
            print(f"outputed to {tmpoutdir}")

    # Matching ground ttruth and predictions.
    pairs = match_true_n_pred_box(df_ttruth, df_pred, IoU_threshold=match_thresh)

    # Asserting status of each match truth/pred: TP, Missclassified, FP (background) and FN (not detected).
    pairs['status'] = "True Positive"
    pairs['status'] = pairs['status'].where(pairs['name_true'] == pairs['name_pred'], 'Missclassified')
    pairs['status'] = pairs['status'].where(pairs['name_true'].notnull(), 'False positive (background)')
    pairs['status'] = pairs['status'].where(pairs['name_pred'].notnull(), 'False negative (not detected)')

    # For each classe (species), counting numbers TP, FP (background), FN (not detected),
    # FN (stemming from missclassification), FP (stemming from missclassification).
    cls_value_counts = get_status_count(pairs)

    if write_outputs:
        tmpoutdir = os.path.join(output_directory, "outcome_counts_per_class.tsv")
        if verbose:
            print(f'Writing the counts of TP, FP and FN to {tmpoutdir}')
        cls_value_counts.to_csv(tmpoutdir, sep='\t')

    # Some general metrics to report in verbose mode
    total_true_labels = pairs.id_true.notnull().sum()
    true_labels_without_matching_preds = pairs.id_pred.isnull().sum()
    perc_detected_animals = 100 - (true_labels_without_matching_preds / total_true_labels * 100)
    perc_correct_class = pairs['is_correct_class'].sum() / pairs.dropna().shape[0] * 100

    if verbose:

        print(f'The test set represents a total of {total_true_labels} specimens.')
        print(f'The model produced {len(tpred["annotations"])} prediction, of which {df_pred.shape[0]} remains after deduplication' +
               ' and removal of oversized bounding boxes.')
        print(f'{total_true_labels - true_labels_without_matching_preds} ({round(perc_detected_animals, 1)}% of the total) ' +
               'of the actual specimens were correcly detected.' +
              f' Of those detected specimens, {int(pairs["is_correct_class"].sum())} (= {round(perc_correct_class, 1)}%) where assigned to the correct species.')
        print( f'Of the predicted labels, {cls_value_counts["False positive background"].sum()} '+
              f'(={round(cls_value_counts["False positive background"].sum() / df_pred.shape[0] * 100, 1)}%) '+
               'where false positive (background, not related to a real specimen)')


    # Adding outcomes on df_ttruth
    df_ttruth = df_ttruth.merge(pairs[pairs['name_true'].notnull()][['id_true', 'score', 'name_pred', 'is_correct_class']], on='id_true')
    df_ttruth['is_detected'] = df_ttruth['is_correct_class'].where(df_ttruth['is_correct_class'].isnull(), 1).fillna(0)

    pairs = pairs.merge(df_ttruth[['id_true', 'bbox']].rename(columns={'bbox': 'bbox_true'}), on='id_true', how='outer')\
                .merge(df_pred[['id_pred', 'bbox']].rename(columns={'bbox': 'bbox_pred'}), on= 'id_pred',  how='outer')

    pairs['box_true'] = pairs['bbox_true'].apply(lambda x: COCObox_2_shapely(x) if isinstance(x, list) else box(0,0,0,0))
    pairs['box_pred'] = pairs['bbox_pred'].apply(lambda x: COCObox_2_shapely(x) if isinstance(x, list) else box(0,0,0,0))
    pairs['IoU_truth_pred']= pairs[['box_true','box_pred']].apply(IoU, axis = 1)
    pairs = pairs.drop(labels=['score_x', 'score_y'], axis=1)

    if draw_n_plot:
        # Drawing the predicted annotations on the pictures
        #------------------------------------------------------------------------------------------------
        print('\n\nDrawing the predicted annotations of the test pictures to support visual verification')
        print('Do not use for testing or for training ! =)')

        draw_coco_bbox(df_pred, os.path.join(output_directory), test_directory,
                    prefix="predicted", line_width=10, fontsize = 150, fontYshift = -125)
        #------------------------------------------------------------------------------------------------

        # Plotting the confusion matrices
        #------------------------------------------------------------------------------------------------
        # 1. CM including only the detected true label

        mcm = confusion_matrix(pairs.dropna().name_true, pairs.dropna().name_pred.fillna('NaN'), labels = pairs.dropna().name_true.unique())
        disp = ConfusionMatrixDisplay(confusion_matrix=mcm, display_labels=pairs.dropna().name_true.unique())
        fig, ax = plt.subplots(figsize=(10,10))
        disp.plot(xticks_rotation='vertical', ax=ax, values_format="d")
        plt.savefig(os.path.join(output_directory, 'cm_onlydetected.svg'), format="svg")

        # 2. CM including only the detected true label, normalized
        # Note: the normalized matrix option is bugged in the plot_confusion_matrix function from sklearn
        # Thus I normalize the matrix here before plotting and don't use the option
        mcm = mcm.astype('float') / mcm.sum(axis=1)[:, np.newaxis] * 100
        mcm = mcm.round(1)
        disp = ConfusionMatrixDisplay(confusion_matrix=mcm, display_labels=pairs.dropna().name_true.unique())
        fig, ax = plt.subplots(figsize=(10,10))
        disp.plot(xticks_rotation='vertical', ax=ax, values_format=".1f")
        plt.savefig(os.path.join(output_directory, 'cm_norm_onlydetected.svg'), format="svg")

        # 3. CM including only the undetected true label (Nan)
        mcm = confusion_matrix(pairs.name_true.fillna('NaN'), pairs.name_pred.fillna('NaN'), labels = pairs.fillna('NaN').name_true.unique())
        disp = ConfusionMatrixDisplay(confusion_matrix=mcm, display_labels=pairs.fillna('NaN').name_true.unique())
        fig, ax = plt.subplots(figsize=(10,10))
        disp.plot(xticks_rotation='vertical', ax=ax, values_format="d")
        plt.savefig(os.path.join(output_directory, 'cm_inclNaN.svg'), format="svg")

        #plot_confusion_matrix(mcm, np.append(pairs.name_true.unique(), 'NaN'),
        #        write=os.path.join(output_directory, "test_results/cm_inclNaN.png"))

        # 4. CM including only the undetected true label (Nan), normalized
        mcm = mcm.astype('float') / mcm.sum(axis=1)[:, np.newaxis] * 100
        mcm = np.nan_to_num(mcm.round(1))
        disp = ConfusionMatrixDisplay(confusion_matrix=mcm, display_labels=pairs.fillna('NaN').name_true.unique())
        fig, ax = plt.subplots(figsize=(10,10))
        disp.plot(xticks_rotation='vertical', ax=ax, values_format=".1f")
        plt.savefig(os.path.join(output_directory, 'cm_norm_inclNaN.svg'), format="svg")

        print("\n---------------Finished Evaluation---------------")

    return pairs, cls_value_counts, df_ttruth, df_pred
